{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODOS/IDEAS:\n",
    "- Just jump through the paper, use my own order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal:\n",
    "\n",
    "The goal of this notebook is to implement the paper: An image is worth 16*16 words: Transformers for image recognition at scale. Paper implementations are a great way to really understand what is happening and to learn how to alter existing models and techniques. To do so, I will go through the paper step by step. Explain the math and translate the architecture to code. After this we will add some extra's to the proposed architecture and paramaters. I Try to make the text as simple as possible for myself to ensure that I understand what I am reading. Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard\n",
    "Transformer directly to images, with the fewest possible modifications. To do so, we split an image\n",
    "into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\n",
    "the model on image classification in supervised fashion.\n",
    "\n",
    "\n",
    "Let's look at the above text, most people are familiar with the paper attention is all you need and the succes of the transformer in nlp. The transformer architecture will be described later. The authors mention that they want to apply this architecture directly to images, without making to many tweaks.\n",
    "This is done by:\n",
    "- Dividing the image into smaller images (patches).\n",
    "- These patches are then provided as a 1D input to a transformer.\n",
    "        (math>?)\n",
    "- Then the model is used for supervised classification (weird sentence). We give images and truth labels. Think cat and dog.\n",
    "\n",
    "After this, the paper mentions that it performs below resnets on a mid-sized dataset. But it performs very well when trained big datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Data + setup\n",
    "\n",
    "- get data\n",
    "- import data -> use appropriate classes\n",
    "- use augmentations etc -> check library I used in HOA.\n",
    "\n",
    "Let's see what the paper has to say about the input data.\n",
    "\n",
    "An overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\n",
    "sequence of token embeddings. To handle 2D images, we reshape the image x ∈ R\n",
    "H×W×C into a sequence of flattened 2D patches xp ∈ R N×(P2·C), where (H, W) is the resolution of the original\n",
    "image, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P2\n",
    "is the resulting number of patches, which also serves as the effective input sequence length for the\n",
    "Transformer. The Transformer uses constant latent vector size D through all of its layers, so we\n",
    "flatten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\n",
    "the output of this projection as the patch embeddings.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about the model overview here:\n",
    "\n",
    "IDEA:\n",
    "- Can add image and surround stuff and use numbers to reference.\n",
    "- Then break it into parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
